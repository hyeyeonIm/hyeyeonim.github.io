<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hyeyeon IM</title>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="style.css">
</head>
<body data-bs-spy="scroll" data-bs-target="#navbar" data-bs-offset="50">

    <!-- Navigation Bar -->
    <nav id="navbar" class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top">
        <div class="container-fluid">
            <a class="navbar-brand" href="#">
                <span class="kr-text">ÏûÑÌòúÏó∞</span>
                <span class="en-text" style="display: none;">Hyeyeon IM</span>
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <!-- Í∏∞Ï°¥ ÎÑ§ÎπÑÍ≤åÏù¥ÏÖò ÎßÅÌÅ¨ -->
                    <li class="nav-item">
                        <a class="nav-link">
                            <span class="kr-text">ÌîÑÎ°úÏ†ùÌä∏</span>
                            <span class="en-text" style="display: none;">Projects</span>
                        </a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link">
                            <span class="kr-text">Í∏∞Ïà†</span>
                            <span class="en-text" style="display: none;">Skills</span>
                        </a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link">
                            <span class="kr-text">Ïó∞ÎùΩÏ≤ò</span>
                            <span class="en-text" style="display: none;">Contact Me</span>
                        </a>
                    </li>
                    <!-- Ïñ∏Ïñ¥ Ï†ÑÌôò Î≤ÑÌäº -->
                    <li class="nav-item">
                        <a class="nav-link" href="index.html">EN</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link active" href="index_KR.html">KR</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Ïó¨Í∏∞Ïóê ÌÜ†Í∏Ä Î≤ÑÌäº Ï∂îÍ∞Ä -->
    <div id="language-toggle" class="position-fixed" style="top: 80px; right: 20px; z-index: 1000;">
        <button onclick="toggleLanguage()" class="btn btn-outline-dark">üåê KR/EN</button>
    </div>

    <!-- Header Section -->
    <header class="header-section text-center text-white d-flex align-items-center justify-content-center">
        <div class="container">
            <div id="header-kr">
                <h1 class="display-3 fw-bold">ÏûÑÌòúÏó∞</h1>
                <p class="lead">Î°úÎ≥¥Ìã±Ïä§Î•º ÌÜµÌïú Îçî ÎÇòÏùÄ ÏÑ∏ÏÉÅ ÎßåÎì§Í∏∞</p>
                <a href="https://drive.google.com/file/d/1uZx7fEgBndUuPchfTb7mfoBMMzswJT6x/view?usp=sharing" target="_blank" class="btn btn-warning btn-lg mt-3">Ïù¥Î†•ÏÑú</a>
            </div>
            <div id="header-en" style="display: none;">
                <h1 class="display-3 fw-bold">Hyeyeon IM</h1>
                <p class="lead">Building a Better World Through Communication in Robotics</p>
                <a href="https://drive.google.com/file/d/1uZx7fEgBndUuPchfTb7mfoBMMzswJT6x/view?usp=sharing" target="_blank" class="btn btn-warning btn-lg mt-3">Resume</a>
            </div>
        </div>
    </header>

    <!-- Projects Section -->
<section id="projects" class="py-5 bg-light">
    <div class="container">
        <h2 class="text-center mb-4">Projects</h2>
        <div class="row">
            <!-- Project Card 1 - Carebuddy -->
            <div class="col-md-4 mb-4">
                <div class="card h-100 shadow-sm" data-bs-toggle="modal" data-bs-target="#projectModal1">
                    <img src="./projects/carebuddy.jpeg" class="card-img-top fixed-ratio" alt="Carebuddy: Automatic Air Purifier Robot">
                    <div class="card-body">
                        <h5 class="card-title">Carebuddy: Automatic Air Purifier Robot</h5>
                        <p class="card-text">September 2023 - May 2024</p>
                    </div>
                </div>
            </div>

            <!-- Project Card 2 - Cartographer Automapping -->
            <div class="col-md-4 mb-4">
                <div class="card h-100 shadow-sm" data-bs-toggle="modal" data-bs-target="#projectModal6">
                    <img src="./projects/carto_main.png" class="card-img-top fixed-ratio" alt="Cartographer Automapping">
                    <div class="card-body">
                        <h5 class="card-title">Cartographer Automapping</h5>
                        <p class="card-text">August 2024</p>
                    </div>
                </div>
            </div>

            <!-- Project Card 3 - Collision Prevention System -->
            <div class="col-md-4 mb-4">
                <div class="card h-100 shadow-sm" data-bs-toggle="modal" data-bs-target="#projectModal2">
                    <img src="./projects/LidarCamera.png" class="card-img-top fixed-ratio" alt="Collision Prevention System: LiDAR-Camera Calibration">
                    <div class="card-body">
                        <h5 class="card-title">Collision Prevention System : LiDAR-Camera Calibration</h5>
                        <p class="card-text">August 2024</p>
                    </div>
                </div>
            </div>

            <!-- Project Card 4 - H-mobility Autocar Project -->
            <div class="col-md-4 mb-4">
                <div class="card h-100 shadow-sm" data-bs-toggle="modal" data-bs-target="#projectModal3">
                    <img src="./projects/hmobility.png" class="card-img-top fixed-ratio" alt="H-mobility Autocar Project">
                    <div class="card-body">
                        <h5 class="card-title">H-mobility Autocar Project</h5>
                        <p class="card-text">July 2024</p>
                    </div>
                </div>
            </div>

            <!-- Project Card 5 - Gachigayo -->
            <div class="col-md-4 mb-4">
                <div class="card h-100 shadow-sm" data-bs-toggle="modal" data-bs-target="#projectModalGachigayo">
                    <img src="./projects/gotogether.png" class="card-img-top fixed-ratio" alt="Gachigayo: AI Conversation Service">
                    <div class="card-body">
                        <h5 class="card-title">Gachigayo: AI Conversation Service</h5>
                        <p class="card-text">August 2023 - September 2023</p>
                    </div>
                </div>
            </div>

            <!-- Project Card 6 - Isaac Sim Project -->
            <div class="col-md-4 mb-4">
                <div class="card h-100 shadow-sm" data-bs-toggle="modal" data-bs-target="#projectModal5">
                    <img src="./projects/isaacsim.png" class="card-img-top fixed-ratio" alt="Isaac Sim Project">
                    <div class="card-body">
                        <h5 class="card-title">Isaac Sim Project</h5>
                        <p class="card-text">August 2024</p>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- --------------------------------------------------------------------------------------------------------------- -->

<!-- Modal for Carebuddy -->
<div class="modal fade" id="projectModal1" tabindex="-1" aria-labelledby="projectModalLabel1" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h5 class="modal-title" id="projectModalLabel1">Carebuddy</h5>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
                <!-- Skill Tags -->
                <div class="mb-3 text-center">
                    <span class="badge bg-warning text-dark mx-1">AMR</span>
                    <span class="badge bg-warning text-dark mx-1">LiDAR SLAM</span>
                    <span class="badge bg-warning text-dark mx-1">Python</span>
                    <span class="badge bg-warning text-dark mx-1">Object Detection</span>
                    <span class="badge bg-warning text-dark mx-1">LSTM</span>
                </div>

                <!-- Brief Description -->
                <h5 class="fw-bold mt-4">Brief Description</h5>
                <p>This project focuses on developing a mobile air purifier robot using LiDAR SLAM and an LSTM-based air quality scheduling system, integrated with object detection capabilities to avoid obstacles.</p>

                <!-- Motivation -->
                <h5 class="fw-bold mt-4">Motivation</h5>
                <p>In today‚Äôs world, air quality management is crucial, especially in urban environments. This project aims to address this challenge by creating a robot that autonomously purifies the air while navigating around obstacles to ensure optimal functionality in various indoor settings.</p>

                <!-- Environment Section -->
                <h5 class="fw-bold mt-4">Scenario</h5>
                <p>In an indoor home environment, the system automatically identifies the space without user intervention, learns the air quality, and manages the air optimally.</p>
                <p>
                    <strong>Robot Characteristics:</strong>
                    <ul>
                        <li><strong>Robot Model:</strong> Omo R1 Mini</li>
                        <li>Utilizes a low-spec board and 2D LiDAR for mapping, which imposes performance limitations.</li>
                    </ul>
                </p>
                <p>
                    <strong>Risk Analysis:</strong>
                    <ul>
                        <li>
                            <strong>Air Quality Data Learning:</strong>
                            <p>The system needs to continuously learn from incoming air quality data. However, hardware limitations prevent the robot from consistently receiving and processing continuous data streams.</p>
                        </li>
                        <li>
                            <strong>Movement:</strong>
                            <p>Continuous movement is necessary for effective air purification, but the robot's battery life poses a constraint, limiting the duration and extent of its operation.</p>
                        </li>
                    </ul>
                </p>

                <!-- Features and Implementation Section -->
                <div class="mt-4">
                    <h5 class="fw-bold">Features and Implementation</h5>

                    <!-- Auto Mapping -->
                    <h6 class="fw-bold">Auto Mapping</h6>
                    <p>Implemented automatic mapping using ROS's gmapping and explore_lite.</p>
                    <p>Due to low-spec hardware, automatic mapping with ROS2 could not be utilized.
                        Instead, automatic mapping was achieved using ROS's gmapping and <code>explore_lite</code> packages.
                        Below is the implementation method of the <code>explore_lite</code> package.
                    </p>
                    <img src="./projects/carebuddy/explore_lite.png" class="project-image mb-3" alt="explore_lite">
                    <p>
                        The automatic mapping feature was implemented by adjusting the path generation parameters through the explore_lite frontier algorithm and tuning the parameters that generate the map based on gmapping's lidar scan data.
                        The following image is the rqt graph of the final automapping implementation on the Omo R1 Mini robot.
                    </p>
                    <img src="./projects/carebuddy/automapping.png" class="project-image mb-3" alt="automapping">

                    <!-- Room Segmentation -->
                    <h6 class="fw-bold">Room Segmentation</h6>
                    <p>Utilized the <code>ipa_room_segmentation</code> package to divide the mapped environment.
                        During this process, the following issues were encountered:
                        <ol>
                            <li>Excessive space segmentation as shown in the first image.</li>
                            <li>Coordinate mismatches of the segmented spaces on the map due to differences in the map's origin.</li>
                        </ol>
                    </p>
                    <!-- Îëê Ïù¥ÎØ∏ÏßÄÎ•º Í∞ôÏùÄ ÌñâÏóê Î∞∞Ïπò -->
                    <div class="row">
                        <div class="col-md-6 mb-3">
                            <div class="ratio ratio-4x3"> <!-- ÏõêÌïòÎäî ÎπÑÏú®Î°ú Î≥ÄÍ≤Ω Í∞ÄÎä• -->
                                <img src="./projects/carebuddy/toomuch.png" class="project-image" alt="seg_issue1">
                            </div>
                        </div>
                        <div class="col-md-6 mb-3">
                            <div class="ratio ratio-4x3"> <!-- ÏõêÌïòÎäî ÎπÑÏú®Î°ú Î≥ÄÍ≤Ω Í∞ÄÎä• -->
                                <img src="./projects/carebuddy/origin_issue.png" class="project-image" alt="seg_issue2">
                            </div>
                        </div>
                    </div>
                    <p>
                        To address these issues, the following solutions were implemented:
                        <ol>
                            <li>Removed adjacent points through clustering.</li>
                            <li>Adjusted the origin in the map's YAML file.</li>
                            <li>Used OpenCV to flip and rotate the map itself to align the map coordinates correctly.</li>
                        </ol>
                        Below is the ROOM ID segmentation correctly positioned on the map.
                    </p>
                    <img src="./projects/carebuddy/room_seg.png" class="project-image" alt="room_seg">

                    <!-- Embedded YouTube Video -->
                    <h5 class="fw-bold mt-4">Automapping and Navigation Demonstration</h5>
                    <div class="ratio ratio-16x9">
                        <iframe src="https://www.youtube.com/embed/A8NuGGdWVhk" title="Automapping and Navigation Demonstration" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                    </div>

                    <!-- Air Quality Analysis -->
                    <h6 class="fw-bold">Air Quality Analysis</h6>
                    <p>Collected air quality data using fine dust sensors and created an air quality prediction model using LSTM.</p>
                    <img src="./projects/carebuddy/LSTM.png" class="project-image mb-3" alt="LSTM">

                    <!-- Web Control -->
                    <h6 class="fw-bold">Web Control</h6>
                    <p>
                        Since actual users of the product may not be familiar with Linux, it is necessary to make it user-friendly.
                        Utilized the <code>rosbridge_suite</code> to include a WebSocket server, allowing web browsers to communicate with ROS.
                        <br>
                        <code>roslibjs:</code> Enables web applications to interact with ROS using the rosbridge protocol.<br>
                        <code>ros2djs:</code> Renders spatial information such as robots and maps, and interactively visualizes ROS topics and messages.
                    </p>
                    <img src="./projects/carebuddy/web.png" class="project-image" alt="web">
                </div>

                <!-- Hardware/Software Section -->
                <div class="mt-4">
                    <h5 class="fw-bold">Hardware/Software</h5>
                    <img src="./projects/carebuddy/background.png" class="project-image" alt="Hardware/Software">
                </div>

                <!-- Structure Section -->
                <div class="mt-4">
                    <h5 class="fw-bold">Structure</h5>
                    <img src="./projects/carebuddy/structure.png" class="project-image" alt="Structure">
                </div>

                <!-- Embedded YouTube Video -->
                <h5 class="fw-bold mt-4">Project Demonstration</h5>
                <div class="ratio ratio-16x9">
                    <iframe src="https://www.youtube.com/embed/uZsxsQaiH3c" title="CareBuddy Demonstration" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                </div>
            </div>
        </div>
    </div>
</div>


<!-- Modal for Cartographer AutoMapping -->
<div class="modal fade" id="projectModal6" tabindex="-1" aria-labelledby="projectModalLabel6" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h5 class="modal-title" id="projectModalLabel6">Cartographer AutoMapping</h5>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">                
                <!-- Skill Tags -->
                <div class="mb-3 text-center">
                    <span class="badge bg-warning text-dark mx-1">AMR</span>
                    <span class="badge bg-warning text-dark mx-1">Auto Mapping</span>
                    <span class="badge bg-warning text-dark mx-1">Jetson Nano</span>
                </div>

                <!-- Brief Description -->
                <h5 class="fw-bold mt-4">Brief Description</h5>
                <p>
                    Implemented <strong>automatic mapping</strong> based on <strong>Cartographer</strong> on <strong>Allbot</strong>, utilizing components such as:
                </p>               
                <h5 class="fw-bold mt-4">Motivation</h5>
                <p>
                    Aiming to perform automatic mapping using <strong>Cartographer</strong>, which offers superior performance compared to <strong>Gmapping</strong>.
                </p>
                <!-- Environment Section -->
                <h5 class="fw-bold mt-4">Environment</h5>
                <strong>Robot Characteristics:</strong>
                <ul>
                    <li><strong>Robot Model:</strong> Allbot</li>
                    <li>Relatively heavy and has a short battery life.</li>
                </ul>
                <p>
                    <strong>Risk Analysis:</strong>
                    <ul>
                        <li>
                            <strong>Sensor Utilization:</strong>
                            <p>Although there are various sensors, camera latency occurs on the Jetson Nano.</p>
                        </li>
                    </ul>
                </p>

                <!-- Features and Implementation Section -->
                <div class="mt-4">
                    <h5 class="fw-bold">Features and Implementation</h5>

                    <!-- Problem -->
                    <h6 class="fw-bold">Problem</h6>
                    <p>
                        To perform auto mapping in ROS, the published map must be received by explore_lite, but Cartographer does not specifically publish a map.
                        The structure of explore_lite is as follows.
                    </p>
                    <img src="./projects/carebuddy/explore_lite.png" class="project-image mb-3" alt="explore_lite">
                    <p>
                        The Cartographer map is not represented merely with values like EMPTY or UNKNOWN (required by explore_lite) but with various occupancy probability values.
                        The structure of Cartographer is as follows.
                    </p>

                    <img src="./projects/carto_auto/cartographer.png" class="project-image mb-3" alt="cartographer">
                    <h6 class="fw-bold">Solution</h6>
                    <p>Therefore, we plan to solve the problem in the following way:
                        <ol>
                            <li>Publish cmap through <code>/submap_list</code>.</li>
                            <li>Convert the published cmap to a standard map.</li>
                            <li>Apply the frontier algorithm to the converted map to enable Auto Mapping.</li>
                        </ol>
                        In other words, receive the map data from the <code>/cmap</code> topic generated by Cartographer, convert it to the map data format used by Gmapping, and then publish it to the <code>/map</code> topic.
                    </p>
                    <img src="./projects/carto_auto/carto_automapping.png" class="project-image mb-3" alt="carto_automapping">

                    <!-- Embedded YouTube Video -->
                    <h5 class="fw-bold mt-4">Project Demonstration</h5>
                    <div class="ratio ratio-16x9">
                        <iframe src="https://www.youtube.com/embed/tmu-C6UV9_4?si=_0EJdR3AdZx8KScN" title="Cartographer AutoMapping Demonstration" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<!-- Modal for LiDAR-Camera Calibration -->
<div class="modal fade" id="projectModal2" tabindex="-1" aria-labelledby="projectModalLabel2" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <!-- Modal Header -->
            <div class="modal-header">
                <h5 class="modal-title" id="projectModalLabel2">LiDAR-Camera Calibration</h5>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>

            <!-- Modal Body -->
            <div class="modal-body">
                <!-- Skill Tags -->
                <div class="mb-4 text-center">
                    <span class="badge bg-warning text-dark mx-1">LiDAR-Camera Calibration</span>
                    <span class="badge bg-warning text-dark mx-1">Object Detection</span>
                    <span class="badge bg-warning text-dark mx-1">Python</span>
                </div>

                <!-- Brief Description -->
                <h5 class="fw-bold mt-4">Brief Description</h5>
                <p>This project focuses on designing an enhanced collision prevention system through precise distance measurements achieved via calibration. Utilized a target-based early fusion method with a checkerboard to perform calibration.</p>

                <!-- Motivation -->
                <h5 class="fw-bold mt-4">Motivation</h5>
                <p>To resolve the distance measurement issues encountered when using LiDAR and Camera independently in the CareBuddy project.</p>

                <!-- Scenario Section -->
                <h5 class="fw-bold mt-4">Scenario</h5>
                <p>
                    <strong>Workflow:</strong> Object Detection through Camera ‚Üí Distance Measurement via LiDAR and Camera ‚Üí Display warning messages upon approaching specific distances.
                </p>

                <!-- Risk Analysis -->
                <h5 class="fw-bold mt-4">Risk Analysis</h5>
                <ul>
                    <li>
                        <strong>Air Quality Data Learning:</strong>
                        <p>The system needs to continuously learn from incoming air quality data. However, hardware limitations prevent the robot from consistently receiving and processing continuous data streams.</p>
                    </li>
                    <li>
                        <strong>Movement:</strong>
                        <p>Continuous movement is necessary for effective air purification, but the robot's battery life poses a constraint, limiting the duration and extent of its operation.</p>
                    </li>
                </ul>

                <!-- Calibration & Distance Measurement Section -->
                <div class="mt-4">
                    <h5 class="fw-bold">Calibration & Distance Measurement</h5>

                    <!-- Camera Calibration (Intrinsic Parameters) -->
                    <h6 class="fw-bold mt-3">Camera Calibration (Intrinsic Parameters)</h6>
                    
                    <!-- Images in the Same Row -->
                    <div class="row">
                        <div class="col-md-6 mb-3">
                            <img src="./projects/calibration/board_fig.png" class="project-image w-100" alt="Intrinsic Calibration Figure">
                        </div>
                        <div class="col-md-6 mb-3">
                            <img src="./projects/calibration/coner.png" class="project-image w-100" alt="Intrinsic Calibration Corners">
                        </div>
                    </div>

                    <p>
                        <strong>Steps:</strong>
                        <ol>
                            <li><strong>Checkerboard Data Collection:</strong> Collected a total of 32 checkerboard images captured from various angles.</li>
                            <li><strong>Checkerboard Corner Detection:</strong> Utilized OpenCV's <code>findChessboardCorners</code> and <code>cornerSubPix</code> to detect corners in the checkerboard images.</li>
                            <li><strong>Intrinsic Parameter Calculation:</strong> Calculated internal parameters (focal lengths <code>fx</code>, <code>fy</code>, principal point <code>cx</code>, <code>cy</code>, and distortion coefficients) using OpenCV's <code>cv2.calibrateCamera</code> with the collected checkerboard images.</li>
                        </ol>
                    </p>

                    <!-- LiDAR-Camera Point Matching -->
                    <h6 class="fw-bold mt-3">LiDAR-Camera Point Matching</h6>
                    <img src="./projects/calibration/point_matching.png" class="project-image mb-3" alt="LiDAR-Camera Point Matching">
                    <p>
                        <strong>Steps:</strong>
                        <ol>
                            <li><strong>LiDAR Measurement Height Check:</strong> Trimmed the foam board to the LiDAR measurement height (11.5cm) to match LiDAR points with camera image coordinates.</li>
                            <li><strong>Placement:</strong> Attached tape at 11.5cm from the LiDAR-measured edges to facilitate point matching.</li>
                            <li><strong>LiDAR-Camera Point Matching:</strong> Matched LiDAR and Camera points according to the marked areas.</li>
                        </ol>
                    </p>

                    <!-- Extrinsic Parameter Calculation -->
                    <h6 class="fw-bold mt-3">Extrinsic Parameter Calculation</h6>
                    <img src="./projects/calibration/parameter.png" class="project-image mb-3" alt="Extrinsic Parameter Calculation">
                    <p>
                        <strong>Steps:</strong>
                        <ol>
                            <li><code>solvePnP</code>: Calculated the rotation matrix (R) and translation vector (T) between the Camera and LiDAR using LiDAR-Camera matching points.</li>
                            <li><code>Rodrigues Transformation</code>: 
                                <ul>
                                    <li>Converted the rotation vector obtained from <code>cv2.solvePnP</code> into a rotation matrix using Rodrigues transformation.</li>
                                    <li>Constructed the transformation matrix to convert LiDAR coordinates to Camera coordinates:</li>
                                </ul>
                                <p><strong>Transformation Matrix:</strong></p>
                                <pre>
    [[R | T]
    [0 | 1]]
                                </pre>
                                <p>
                                    - <strong>R:</strong> Rotation Matrix<br>
                                    - <strong>T:</strong> Translation Vector
                                </p>
                            </li>
                        </ol>
                    </p>

                    <!-- First Data Overlay -->
                    <h6 class="fw-bold mt-3">First Data Overlay</h6>
                    <img src="./projects/calibration/overlay1.png" class="project-image mb-3" alt="First Data Overlay">
                    <p>
                        <strong>Issue:</strong> An additional line of LiDAR points appeared on the ground.<br>
                        <strong>Solution:</strong>
                        <ol>
                            <li><strong>Adjusted YDLiDAR Launch File Parameters:</strong>
                                <ul>
                                    <li><code>ranges</code>: Distance values detected by the LiDAR sensor.</li>
                                    <li><code>angle_min</code>, <code>angle_max</code>: Range of measurement angles.</li>
                                    <li><code>angle_increment</code>: Increment between angles.</li>
                                </ul>
                                <p><strong>Result</strong> : No change.</p>
                            </li>
                            <li><strong>LiDAR Position Adjustment:</strong>
                                <p>When LiDAR projects onto a flat surface like the ground, reflected signals may be detected slightly off the original point.<br>
                                <strong>Solution</strong> Elevated the LiDAR position to mitigate the issue.</p>
                            </li>
                        </ol>
                    </p>

                    <!-- Data Overlay -->
                    <h6 class="fw-bold mt-3">Data Overlay</h6>
                    <img src="./projects/calibration/overlay2.png" class="project-image mb-3" alt="Data Overlay">
                </div>

                <!-- Object Detection & Distance Measurement Section -->
                <div class="mt-4">
                    <h5 class="fw-bold">Object Detection & Distance Measurement</h5>

                    <!-- Object Detection -->
                    <h6 class="fw-bold mt-3">Object Detection</h6>
                    <p>
                        Utilized the YOLOv8 model (<code>yolov8n.pt</code>) from the COCO Dataset via Ultralytics.
                    </p>

                    <!-- LiDAR-Based Distance Measurement -->
                    <h6 class="fw-bold mt-3">LiDAR-Based Distance Measurement</h6>
                    <p>
                        Measured distance based on the front-most LiDAR point of the detected object.
                        <ol>
                            <li>Converted polar coordinates to Cartesian coordinates using filtered distances (<code>ranges</code>) and angles (<code>angles</code>).</li>
                            <li>Calculated Euclidean distance.</li>
                        </ol>
                    </p>

                    <!-- Camera-Based Distance Measurement -->
                    <h6 class="fw-bold mt-3">Camera-Based Distance Measurement</h6>
                    <p>
                        1) <strong>Focal Length Extraction:</strong> Obtained internal parameters through checkerboard calibration.<br>
                        2) <strong>Object Height Measurement in Image:</strong> Measured the height of the detected object in pixels using the bounding box from the YOLO model.<br>
                        3) <strong>Actual Object Height Definition:</strong> Defined the actual height of the teddy bear to be recognized as 0.2m.<br>
                        4) <strong>Distance Calculation:</strong>  
                        <pre>
Distance = (Focal Length * Actual Height) / Bounding Box Height
                        </pre>
                    </p>

                    <!-- Measurement Method Differences Based on Distance -->
                    <h6 class="fw-bold mt-3">Measurement Method Differences Based on Distance</h6>
                    <p>
                        <strong>Issues:</strong>
                        <ol>
                            <li>2D LiDAR can only accurately measure distance if the detected object is parallel.</li>
                            <li>The detection ranges of LiDAR and Camera differ.</li>
                        </ol>
                        <strong>Solutions:</strong>
                        <ol>
                            <li>For distances below 0.2m, objects are not simultaneously detected by both sensors.<br>
                                <strong>Action:</strong> Use Camera-only distance measurement for objects under 0.2m.
                            </li>
                            <li>For distances below 0.5m, average value errors occur.<br>
                                <strong>Action:</strong> Calculate the average only if the error is below 0.05m.
                            </li>
                        </ol>
                    </p>
                </div>

                <!-- Distance Measurement & Collision Prevention System Section -->
                <div class="mt-4">
                    <h5 class="fw-bold">Distance Measurement & Collision Prevention System</h5>
                    <p>
                        1) <strong>0.50m ‚â§ Object ‚â§ 0.70m:</strong> When an object is between 0.5m and 0.7m, display "<strong>CAUTION! Object Approaching!</strong>"<br>
                        2) <strong>Object &lt; 0.50m:</strong> When an object is closer than 0.5m, display "<strong>WARNING! Immediate Action Required!</strong>"
                    </p>
                </div>

                <!-- Embedded YouTube Video -->
                <h5 class="fw-bold mt-4">Project Demonstration</h5>
                <div class="ratio ratio-16x9 mb-4">
                    <iframe src="https://www.youtube.com/embed/p3hiAyhQx1o" title="Calibration Demonstration" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                </div>
            </div>
        </div>
    </div>
</div>


<!-- Modal for H-mobility Autocar Project -->
<div class="modal fade" id="projectModal3" tabindex="-1" aria-labelledby="projectModalLabel3" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <!-- Modal Header -->
            <div class="modal-header">
                <h5 class="modal-title" id="projectModalLabel3">H-mobility Autocar Project</h5>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>

            <!-- Modal Body -->
            <div class="modal-body">
                <!-- Skill Tags -->
                <div class="mb-3 text-center">
                    <span class="badge bg-warning text-dark mx-1">ROS2</span>
                    <span class="badge bg-warning text-dark mx-1">Object Detection</span>
                    <span class="badge bg-warning text-dark mx-1">Motor Control</span>
                    <span class="badge bg-warning text-dark mx-1">LiDAR</span>
                    <span class="badge bg-warning text-dark mx-1">Python</span>
                </div>

                <!-- Brief Description -->
                <h5 class="fw-bold mt-4">Brief Description</h5>
                <p>
                    Designed a control algorithm that adjusts the vehicle's tilt by training on collected lane data and developed a lane recognition model.
                </p>

                <!-- Scenario Section -->
                <h5 class="fw-bold mt-4">Scenario</h5>
                <p>
                    Utilized a child-friendly vehicle equipped with cameras and LiDAR to perform autonomous driving through indoor lane recognition and traffic light detection.
                </p>
                <p>
                    <strong>Characteristics:</strong>
                </p>
                <ul>
                    <li><strong>Hardware:</strong> 220V output battery, SMPS, motor driver, Arduino Mega 2560, potentiometer, Logitech C270 webcam, RPLiDAR</li>
                </ul>

                <!-- Lane Recognition -->
                <h6 class="fw-bold mt-3">Lane Recognition</h6>
                <ol>
                    <li>
                        <strong>Problem 1:</strong> The steering angle abruptly switched between -7 and 7 degrees based on a step function with a lane slope of 0.
                        <ul>
                            <li>
                                <strong>Solution 1:</strong>
                                <ul>
                                    <li>Implemented a sine function to smooth the transition of steering changes.</li>
                                    <li>Added <code>target_point</code> and <code>car_center_point</code> to adjust the steering angle based on the <code>target_slope</code>, maintaining it around 0.</li>
                                </ul>
                            </li>
                        </ul>
                    </li>
                    <li>
                        <strong>Problem 2:</strong> The vehicle drifted to the right lane.
                        <ul>
                            <li>
                                <strong>Solution 2:</strong> Adjusted the <code>car_center_point</code> within the ROI to correct the lane positioning.
                            </li>
                        </ul>
                    </li>
                    <li>
                        <strong>Problem 3:</strong> Sudden changes in steering angle occurred during driving.
                        <ul>
                            <li>
                                <strong>Solution 3:</strong> Implemented a limit to the steering angle change, restricting it to a maximum of 3 degrees per adjustment.
                            </li>
                        </ul>
                    </li>
                </ol>

                <img src="./projects/hmobility/roboflow.png" class="project-image mb-3" alt="Roboflow Dataset">


                <!-- Control Algorithm -->
                <h6 class="fw-bold mt-3">Control Algorithm</h6>
                <ol>
                    <li>
                        <strong>Problem 1:</strong> The steering angle abruptly switched between -7 and 7 degrees based on a step function with a lane slope of 0.
                        <ul>
                            <li>
                                <strong>Solution 1:</strong>
                                <ul>
                                    <li>Implemented a sine function to smooth the transition of steering changes.</li>
                                    <li>Added <code>target_point</code> and <code>car_center_point</code> to adjust the steering angle based on the <code>target_slope</code>, maintaining it around 0.</li>
                                </ul>
                            </li>
                        </ul>
                    </li>
                    <li>
                        <strong>Problem 2:</strong> The vehicle drifted to the right lane.
                        <ul>
                            <li>
                                <strong>Solution 2:</strong> Adjusted the <code>car_center_point</code> within the ROI to correct the lane positioning.
                            </li>
                        </ul>
                    </li>
                    <li>
                        <strong>Problem 3:</strong> Sudden changes in steering angle occurred during driving.
                        <ul>
                            <li>
                                <strong>Solution 3:</strong> Implemented a limit to the steering angle change, restricting it to a maximum of 3 degrees per adjustment.
                            </li>
                        </ul>
                    </li>
                </ol>

                <div class="row justify-content-center">
                    <div class="col-lg-8 mb-3">
                        <div class="ratio ratio-9x16">
                            <video class="rounded project-video" controls>
                                <source src="./projects/hmobility/autocar_demo.mp4" type="video/mp4">
                            </video>
                        </div>
                    </div>
                </div>

            </div>
        </div>
    </div>
</div>


<!-- Modal for Gachigayo Project -->
<div class="modal fade" id="projectModalGachigayo" tabindex="-1" aria-labelledby="projectModalLabelGachigayo" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <!-- Modal Header -->
            <div class="modal-header">
                <h5 class="modal-title" id="projectModalLabelGachigayo">Gachigayo: AI Conversation Service</h5>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>

            <!-- Modal Body -->
            <div class="modal-body">
                <!-- Skill Tags -->
                <div class="mb-3 text-center">
                    <span class="badge bg-warning text-dark mx-1">Prompt Engineering</span>
                    <span class="badge bg-warning text-dark mx-1">Flask</span>
                    <span class="badge bg-warning text-dark mx-1">AWS</span>
                    <span class="badge bg-warning text-dark mx-1">GitHub</span>
                    <span class="badge bg-warning text-dark mx-1">TTS</span>
                    <span class="badge bg-warning text-dark mx-1">STT</span>
                    <span class="badge bg-warning text-dark mx-1">GPT API</span>
                </div>

                <!-- Brief Description -->
                <h5 class="fw-bold mt-4">Brief Description</h5>
                <p>
                    Developed an AI-based conversational transportation guidance service tailored for the elderly facing challenges with smartphone usage. The service prioritizes user convenience and safety by providing comprehensive information on various transportation modes.
                </p>

                <!-- Motivation -->
                <h5 class="fw-bold mt-4">Motivation</h5>
                <p>
                    The transportation service issues faced by the elderly in our society are an undeniable reality. According to studies by the Korea Institute for Health and Social Affairs and the Ministry of Health and Welfare, the elderly rely on public transportation despite facing challenges in accessing digital information. While services like the 100-won taxi exist, they fall short of meeting actual needs.
                </p>
                <p>
                    The rapid pace of digital technology adoption has led to a widening information gap for the elderly. They encounter difficulties in using smartphones and computers in their daily lives, including when utilizing transportation modes like buses and subways. Specifically, finding route information, schedules, and transfer details online poses significant barriers for the elderly.
                </p>
                <p>
                    To fundamentally address these issues, we proposed the 'Voice Guidance Service for the Elderly.' This service is designed to resolve the challenges the elderly face when using transportation, aiming to make their transportation experiences more convenient and safe.
                </p>

                <!-- Project Introduction -->
                <h6 class="fw-bold mt-3">Project Introduction</h6>
                <p>
                    <strong>Gachigayo</strong> is an AI-driven conversational service designed to provide specialized transportation information for the elderly who are not well-versed in digital technologies. This service emphasizes user convenience and safety, offering detailed information on various transportation methods.
                </p>
                <ul>
                    <li><strong>City/Intercity Travel:</strong> Provides bus, subway, and train route information along with taxi call connection services.</li>
                    <li><strong>Location Information:</strong> Allows users to check operating hours of destinations and receive notifications about closing times.</li>
                    <li><strong>Weather Information:</strong> Offers current and forecasted weather updates, recommends taxis during severe weather conditions, and alerts users about weather-related precautions.</li>
                </ul>
                <p>
                    To utilize this service, user consent for location information is required. By accurately identifying the departure and destination points through prompted queries, the service aims to deliver precise transportation information.
                </p>

                <!-- Market Research -->
                <h6 class="fw-bold mt-3">Market Research</h6>
                <p>
                    According to the Korea Institute for Health and Social Affairs, the primary modes of transportation for the elderly are buses and subways, with a high usage rate of 68.2%, particularly among the 70-74 age group.
                </p>
                <p>
                    Based on the 2020 survey by the Ministry of Health and Welfare, 71.2% of the elderly use public transportation when going out. However, since most information services are online-centric, 74.1% of the elderly find it challenging. Moreover, they face difficulties in using digital devices in daily life.
                </p>
                <p>
                    Despite the accessibility of public transportation, the rapid digitalization has exacerbated the inconvenience for the elderly. The 2018 Information Gap Survey by the National Information Society Agency revealed that the digital literacy of the elderly is only half of the overall average. In an interview with Daily Good News, Mr. Park (80) stated, "I can only go to places I know, and it's burdensome to ask bus drivers for directions."
                </p>
                <p>
                    According to a Financial News article, as public transportation services become more digitalized, the elderly are struggling to adapt to these changes. For instance, Mr. A (late 50s) finds the smartphone app-based taxi booking unfamiliar, leading to long waits on the street. Similarly, Mr. C (71) has difficulty booking trains digitally and resorts to visiting stations or making reservations by phone.
                </p>
                <p>
                    Nowadays, most bus and subway information is available online, but approximately 74.1% of the elderly find it difficult to obtain such online information. In conclusion, the digitalization of public transportation services has posed multiple challenges for the elderly in adapting to these changes, reflecting a lack of efficient services tailored for them despite their active use of public transportation.
                </p>

                <!-- Key Features and Implementation -->
                <h6 class="fw-bold mt-3">Key Features and Implementation</h6>
                <p>
                    Although the smartphone usage capability of the elderly is steadily increasing, it remains limited to phone calls and video watching. Therefore, **Gachigayo** aims to provide a convenient service for the elderly with low digital literacy by developing an AI voice recognition-based conversational service.
                </p>
                <ul>
                    <li><strong>Feature 1: City/Intercity Travel Route Guidance</strong>
                        <ul>
                            <li>Provides bus, subway, and train route information along with taxi call connection services.</li>
                        </ul>
                    </li>
                    <li><strong>Feature 2: Weather Information Guidance</strong>
                        <ul>
                            <li>Offers current weather information, recommends taxis during severe weather conditions, and alerts users about weather-related precautions.</li>
                        </ul>
                    </li>
                    <li><strong>Feature 3: SMS Service</strong>
                        <ul>
                            <li>Sends detailed route information and, for intercity travel, includes Koreail/Bus Terminal numbers in the messages.</li>
                            <li>
                                <img src="./projects/gotogether/call_post_message.png" class="project-image mb-3" alt="Post-Call Message Example">
                            </li>
                        </ul>
                    </li>
                </ul>

                <!-- Workflow Diagram -->
                <h6 class="fw-bold mt-3">Workflow</h6>
                <ol>
                    <li>Converts speech to text using Google STT.</li>
                    <li>Extracts departure and destination information using GPT prompting.</li>
                    <li>Searches routes using Google Maps API.</li>
                    <li>Generates explanatory text using GPT prompting.</li>
                    <li>Converts text to speech using Google TTS.</li>
                    <li>Provides map and text guidance messages after call termination.</li>
                </ol>

                <!-- Revenue Model -->
                <h6 class="fw-bold mt-3">Revenue Model</h6>
                <ul>
                    <li>Implemented a filial piety plan targeting adult children to support their elderly parents.</li>
                    <li>
                        <img src="./projects/gotogether/filial_piety_plan.png" class="project-image mb-3" alt="Filial Piety Plan">
                    </li>
                </ul>

                <!-- Embedded YouTube Video -->
                <h5 class="fw-bold mt-4">Project Demonstration</h5>
                <div class="ratio ratio-16x9 mb-4">
                    <iframe src="https://www.youtube.com/embed/S4wqVm0MXU8" title="Gachigayo - AI Conversation Service" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                </div>

                <!-- Landing Page Link -->
                <p class="text-center">
                    <a href="https://main--cosmic-pixie-da0689.netlify.app/" class="btn btn-primary" target="_blank" rel="noopener noreferrer">Visit Landing Page</a>
                </p>
            </div>
        </div>
    </div>
</div>


<!-- Modal for Isaac Sim Project -->
<div class="modal fade" id="projectModal5" tabindex="-1" aria-labelledby="projectModalLabel5" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <!-- Modal Header -->
            <div class="modal-header">
                <h5 class="modal-title" id="projectModalLabel5">Isaac Sim Project</h5>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>

            <!-- Modal Body -->
            <div class="modal-body">
                <!-- Skill Tags -->
                <div class="mb-3 text-center">
                    <span class="badge bg-warning text-dark mx-1">Omniverse</span>
                    <span class="badge bg-warning text-dark mx-1">Isaac Sim</span>
                    <span class="badge bg-warning text-dark mx-1">Dynamics</span>
                </div>

                <!-- Brief Description -->
                <h5 class="fw-bold mt-4">Brief Description</h5>
                <p>
                    Configured the <strong>physics_step</strong> of the Franka robot to establish a Pick & Place environment.
                </p>

                <!-- Motivation -->
                <h5 class="fw-bold mt-4">Motivation</h5>
                <p>
                    Aimed to implement the <strong>physics_step</strong> of a joint robot within the simulation environment of Omniverse Isaac Sim.
                </p>

                <!-- Adding Objects and Configuring Franka's physics_step -->
                <h6 class="fw-bold mt-3">Adding Objects and Configuring Franka's physics_step</h6>
                <ol>
                    <li>
                        <strong>Background:</strong> Utilized <code>cafe_interior.usdz</code> as the environment background.
                        <div class="row">
                            <div class="col-md-6 mb-3">
                                <img src="./projects/franka/add_cafe.png" class="project-image w-100" alt="Cafe Interior">
                            </div>
                        </div>
                        <strong>Franka's Object Interaction:</strong> Downloaded a <code>.usdz</code> file from Sketchfab that depicts Franka performing pick-up actions.
                    </li>
                    <li>
                        <strong>Adding the Cafe:</strong>
                        <ul>
                            <li><code>__init__:</code> Imported the cafe <code>.usdz</code> file.</li>
                            <li><code>def add_background:</code>
                                <ul>
                                    <li>Set the position, rotation, and scale of the cafe.</li>
                                    <li>Assigned physical properties to the cafe objects.</li>
                                </ul>
                            </li>
                            <li><code>def setup_scene:</code> Added the background to the simulation scene.</li>
                        </ul>
                        <div class="row">
                            <div class="col-md-6 mb-3">
                                <img src="./projects/franka/add_lights.png" class="project-image w-100" alt="Adding Lights">
                            </div>
                        </div>
                    </li>
                    <li>
                        <strong>Adding Lighting:</strong>
                        <ul>
                            <li><code>def add_lights:</code>
                                <ul>
                                    <li>Specified the positions of the lights.</li>
                                    <li>Defined light properties such as radius, intensity, and location before adding them to the stage.</li>
                                </ul>
                            </li>
                            <li><code>def setup_scene:</code> Added the lighting setup to the simulation scene.</li>
                        </ul>
                        <div class="row">
                            <div class="col-md-6 mb-3">
                                <img src="./projects/franka/add_franka.png" class="project-image w-100" alt="Adding Franka">
                            </div>
                        </div>
                    </li>
                    <li>
                        <strong>Adding Franka:</strong>
                        <ul>
                            <li><code>def setup_scene:</code> Imported the Franka robot into the simulation.</li>
                            <li><code>setup_post_load:</code>
                                <ul>
                                    <li>Configured the <code>PickPlaceController</code> for Franka.</li>
                                </ul>
                            </li>
                            <li><code>physics_step:</code>
                                <ul>
                                    <li>Set the positions where Franka picks up the tray and moves it.</li>
                                </ul>
                            </li>
                        </ul>
                        <div class="row">
                            <div class="col-md-6 mb-3">
                                <img src="./projects/franka/tray.png" class="project-image uniform-height" alt="Tray">
                            </div>
                            <div class="col-md-6 mb-3">
                                <img src="./projects/franka/add_tray.png" class="project-image uniform-height" alt="Adding Tray">
                            </div>
                        </div>
                    </li>
                </ol>

                <!-- Embedded YouTube Video -->
                <h5 class="fw-bold mt-4">Project Demonstration</h5>
                <div class="ratio ratio-16x9 mb-4">
                    <iframe src="https://www.youtube.com/embed/pqCttCxLD_Q?si=zn5HWuSSl0qg3Ntu" title="Isaac Sim - Franka" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                </div>
            </div>
        </div>
    </div>
</div>




<!-- Modal for ÎùºÏù∏Ìä∏Î†àÏù¥ÏÑú -->
<div class="modal fade" id="projectModal6" tabindex="-1" aria-labelledby="projectModalLabel6" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h5 class="modal-title" id="projectModalLabel6">ÎùºÏù∏Ìä∏Î†àÏù¥ÏÑú</h5>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
                <p><strong>Project Period:</strong> 2023.09 - 2023.10</p>
                <p>Ïπ¥Î©îÎùºÎ•º ÌÜµÌï¥ Ï∞®ÏÑ†ÏùÑ Ïù∏ÏãùÌïòÏó¨ ÏûêÏú®Ï£ºÌñâÌïòÎäî ÎùºÏù∏Ìä∏Î†àÏù¥ÏÑú ÌîÑÎ°úÏ†ùÌä∏. Ïπ¥Î©îÎùºÎ°ú Î∞õÏùÄ Ïù¥ÎØ∏ÏßÄÎ•º ÌïôÏäµÌïòÏó¨ Ï∞®ÏÑ†ÏùÑ Ïù∏Ïãù.</p>
                    <!-- Skill Tags -->
                    <div class="mb-3 text-center">
                        <span class="badge bg-warning text-dark mx-1">Deep Learning</span>
                        <span class="badge bg-warning text-dark mx-1">TensorFlow</span>
                        <span class="badge bg-warning text-dark mx-1">Python</span>
                        <span class="badge bg-warning text-dark mx-1">SLAM</span>
                        <span class="badge bg-warning text-dark mx-1">Object Detection</span>
                    </div>
    
                    <!-- Brief Description -->
                    <h5 class="fw-bold mt-4">Brief Description</h5>
                    <p>This project focuses on developing a mobile air purifier robot using LiDAR SLAM and LSTM-based air quality scheduling, with object detection capabilities to avoid obstacles.</p>
    
                    <!-- Motivation -->
                    <h5 class="fw-bold mt-4">Motivation</h5>
                    <p>In today‚Äôs world, air quality management is crucial, especially in urban environments. This project aims to address this challenge by creating a robot that autonomously purifies the air while navigating around obstacles to ensure optimal functionality in various indoor settings.</p>
    
                    <!-- Images Section -->
                    <div class="mt-4">
                        <h5 class="fw-bold">Experimental Setup</h5>
                        <img src="path_to_your_image_1.jpg" class="img-fluid mb-3" alt="Carebuddy Image 1">
    
                        <h5 class="fw-bold mt-4">Results</h5>
                        <img src="path_to_your_image_2.jpg" class="img-fluid mb-3" alt="Carebuddy Image 2">
                        <img src="path_to_your_image_3.jpg" class="img-fluid mb-3" alt="Carebuddy Image 3">
                    </div>
            </div>
        </div>
    </div>
</div>


<!-- --------------------------------------------------------------------------------------------------------------- -->

    <!-- Skills Section -->
    <section id="skills" class="py-5">
        <div class="container">
            <h2 class="text-center mb-4">
                <span class="kr-text">Í∏∞Ïà†</span>
                <span class="en-text" style="display: none;">Skills</span>
            </h2>
            <div class="row text-center">
                <div class="col-md-4">
                    <h5>Robotics</h5>
                    <p>ROS, Linux, Git, Isaac Sim, Gazebo, Computer Vision, Machine Learning</p>
                </div>
                <div class="col-md-4">
                    <h5>Programming</h5>
                    <p>C++, Python, MATLAB</p>
                </div>
                <div class="col-md-4">
                    <h5>
                        <span class="kr-text">Í∏∞Í≥Ñ Í≥µÌïô</span>
                        <span class="en-text" style="display: none;">Mechanical Engineering</span>
                    </h5>
                    <p>AutoCAD, NX</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Contact Section -->
    <section id="contact" class="py-5 text-center">
        <div class="container">
            <h2>
                <span class="kr-text">Ïó∞ÎùΩÏ≤ò</span>
                <span class="en-text" style="display: none;">Contact Me</span>
            </h2>
            <img src="haley.jpeg" class="img-fluid rounded-circle mb-3" alt="Hyeyeon IM" style="width: 200px; height: 200px; object-fit: cover;">
            <p>
                <span class="kr-text">ÏûÑÌòúÏó∞, Î°úÎ≥¥Ìã±Ïä§ ÏóîÏßÄÎãàÏñ¥</span>
                <span class="en-text" style="display: none;">Hyeyeon IM, Robotics Engineer</span>
            </p>
            <div>
                <a href="https://github.com/hyeyeonim" target="_blank" class="btn btn-dark me-2">GitHub</a>
                <a href="mailto:hyeyeonim@gmail.com" class="btn btn-dark me-2">Email</a>
                <a href="https://linkedin.com/in/hyeyeonim" target="_blank" class="btn btn-dark">LinkedIn</a>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer class="bg-dark text-white text-center py-3"></footer>

    <!-- Bootstrap JS and Popper.js -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"></script>
    <script>
    function toggleLanguage() {
        const krElements = document.querySelectorAll('.kr-text');
        const enElements = document.querySelectorAll('.en-text');
        const headerKr = document.getElementById('header-kr');
        const headerEn = document.getElementById('header-en');

        krElements.forEach(el => {
            el.style.display = el.style.display === 'none' ? 'inline' : 'none';
        });
        enElements.forEach(el => {
            el.style.display = el.style.display === 'none' ? 'inline' : 'none';
        });
        
        headerKr.style.display = headerKr.style.display === 'none' ? 'block' : 'none';
        headerEn.style.display = headerEn.style.display === 'none' ? 'block' : 'none';
    }
    </script>
</body>
</html>